{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11382502,"sourceType":"datasetVersion","datasetId":7119570},{"sourceId":11394442,"sourceType":"datasetVersion","datasetId":7136127}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":4627.410374,"end_time":"2025-04-14T00:59:41.668596","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-13T23:42:34.258222","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"80a61e23","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport random\nfrom torchvision import models, datasets\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nimport sklearn.metrics as metric\nimport shutil, glob\nfrom tqdm import tqdm\nfrom PIL import Image\nimport pickle","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":10.381037,"end_time":"2025-04-13T23:42:48.704987","exception":false,"start_time":"2025-04-13T23:42:38.323950","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"290765d6","cell_type":"code","source":"import cv2\nimport numpy as np\n\n\ndef img_range(x):\n    y = (x + abs(x.min()))\n    x = y / y.max()\n    x = x * 255\n    x = x.astype(np.uint8)\n    return x\n\n\ndef DFT(img, offset=11, mode='HP'):\n    dft = cv2.dft(np.float32(img), flags=cv2.DFT_COMPLEX_OUTPUT)\n    dft_shift = np.fft.fftshift(dft)\n    rows, cols = img.shape\n    crow, ccol = rows // 2, cols // 2\n    if mode == 'LP':\n        mask = np.zeros((rows, cols, 2), np.uint8)\n        mask[crow - offset:crow + offset, ccol - offset:ccol + offset] = 1\n    if mode == 'HP':\n        mask = np.ones((rows, cols), np.uint8)*255\n        x, y = crow - 3*offset//2, ccol - 3*offset//2\n        ###################\n        mask[:, ccol - offset//2:ccol + offset//2] = 0\n        mask[crow - offset//2:crow + offset//2, :] = 0\n        cv2.rectangle(mask, (x, y), (rows-x, cols-y), 0, -1)\n        mask = mask//255\n        mask = np.dstack([mask, mask])\n        ######################\n\n    # apply mask and inverse DFT\n    dft_shift_masked = dft_shift * mask\n    inv_masked = np.fft.ifftshift(dft_shift_masked)\n    imginv_masked = cv2.idft(inv_masked)\n    img_dft = cv2.magnitude(imginv_masked[:, :, 0], imginv_masked[:, :, 1])\n    return img_dft\n","metadata":{"papermill":{"duration":0.011594,"end_time":"2025-04-13T23:42:48.719551","exception":false,"start_time":"2025-04-13T23:42:48.707957","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"be433ddd","cell_type":"code","source":"class elpv(torch.utils.data.Dataset):\n    def __init__(self, path, mode='train', types=['mono', 'poly']):\n\n        self.mode = mode\n        self.type = types\n        self.path = path\n        # self.data_path = os.path.join(path, 'train') if mode == 'train' else os.path.join(path, 'val')\n        self.infos_path = os.path.join(path, f\"elpv_infos_{types}_train.pkl\") \\\n                            if mode == 'train' else os.path.join(path, f\"elpv_infos_{types}_test.pkl\")\n\n        with open(self.infos_path, 'rb') as f:\n            self.samples = pickle.load(f)\n        \n        self.to_tensor = transforms.ToTensor()\n\n        self.transform_t_geometric = transforms.Compose([\n            transforms.Resize((288,288)),\n            transforms.RandomHorizontalFlip(0.5),\n            transforms.RandomVerticalFlip(0.2),\n        ])\n\n        self.transform_t_colour = transforms.Compose([\n            transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.7, 1.3))\n        ])\n\n        self.resized_v = transforms.Resize((288,288))\n        self.normalized_tv_img = transforms.Normalize(mean=[0.5968], std=[0.0977])\n        self.normalized_tv_filter = transforms.Normalize(mean=[0.1], std=[0.0984])\n        \n\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.path, self.samples[index][\"Path\"])\n        lab = int(self.samples[index][\"Class\"])\n        \n        if self.samples[index][\"Type\"] != self.type:\n            raise f\"TypeMatchError: Having issue in {self.samples[index]}\"\n\n        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        filter_image = img_range(DFT(image, offset=5, mode='HP'))\n\n        if self.mode == \"train\":\n            image = self.to_tensor(image)\n            filter_image = self.to_tensor(filter_image)\n            img_cat_filter = torch.cat([image, filter_image], dim=0)\n            img_cat_filter_geo = self.transform_t_geometric(img_cat_filter)\n            filter_image = img_cat_filter_geo[1].unsqueeze(0)\n            image_colour_aug = self.transform_t_colour(img_cat_filter_geo[0].unsqueeze(0))\n            image = self.normalized_tv_img(image_colour_aug)\n            filter_image = self.normalized_tv_filter(filter_image)\n            # image = image_colour_aug\n        else:\n            image = self.to_tensor(image)\n            filter_image = self.to_tensor(filter_image)\n            image = self.resized_v(image)\n            filter_image = self.resized_v(filter_image)\n            image = self.normalized_tv_img(image)\n            filter_image = self.normalized_tv_filter(filter_image)\n\n        label = torch.zeros(2)\n        label[lab] = 1\n\n        return image, filter_image, label, self.samples[index][\"Path\"]\n    \n    def __len__(self):\n        return len(self.samples)","metadata":{"papermill":{"duration":0.012775,"end_time":"2025-04-13T23:42:48.734922","exception":false,"start_time":"2025-04-13T23:42:48.722147","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"d27e5018","cell_type":"code","source":"mod = 'mono'\ntrain_data = elpv(f\"/kaggle/input/elpv-and-pvel-ad-dataset/elpv_{mod}\", mode='train',types=mod)\nval_data = elpv(f\"/kaggle/input/elpv-and-pvel-ad-dataset/elpv_{mod}\", mode='val', types=mod)","metadata":{"papermill":{"duration":0.018792,"end_time":"2025-04-13T23:42:48.756184","exception":false,"start_time":"2025-04-13T23:42:48.737392","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"a922ab5a","cell_type":"code","source":"train_loader=torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True , num_workers = 4)\nval_loader=torch.utils.data.DataLoader(val_data, batch_size=64, shuffle=True , num_workers = 4)","metadata":{"papermill":{"duration":0.008855,"end_time":"2025-04-13T23:42:48.777080","exception":false,"start_time":"2025-04-13T23:42:48.768225","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"720a0ddd","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch.utils.model_zoo as model_zoo\n\nmodel_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n}\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, dilation=dilation,\n                               padding=dilation, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes,\n                 stride=1, downsample=None, groups=1, base_width=64,\n                 dilation=1, norm_layer=None,\n                 activation=nn.ReLU(inplace=True), residual_only=False):\n\n        super(BasicBlock, self).__init__()\n        self.residual_only = residual_only\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and '\n                             'base_width=64')\n        # Both self.conv1 and self.downsample layers downsample the input\n        # when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride, dilation=dilation)\n        self.bn1 = norm_layer(planes)\n        self.act = activation\n        self.conv2 = conv3x3(planes, planes, dilation=dilation)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.act(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        if self.residual_only:\n            return out\n        out = out + identity\n        out = self.act(out)\n\n        return out\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False,\n                     dilation=dilation)\n\n\nclass DCMAF(nn.Module):\n    def __init__(self, num_channel, out_channel):\n        super().__init__()\n\n        # todo add convolution here\n        self.pool = nn.AdaptiveAvgPool2d(1)  # [B, C, 1, 1]\n\n        self.conv1 = nn.Conv2d(num_channel, out_channel, kernel_size=1)\n        self.conv2 = nn.Conv2d(num_channel, out_channel, kernel_size=1)\n        self.conv3 = nn.Conv2d(out_channel, num_channel, kernel_size=1)\n        self.conv4 = nn.Conv2d(out_channel, num_channel, kernel_size=1)\n        self.activation = nn.Sigmoid()\n\n    def forward(self, image, filter_img):\n        image = self.pool(image)\n        filter_img = self.pool(filter_img)\n\n        image = F.relu(self.conv1(image))\n        filter_img = F.relu(self.conv2(filter_img))\n\n        image = self.conv3(image)\n        filter_img = self.conv4(filter_img)\n\n        diff = image - filter_img\n        weight = self.activation(diff)\n        w1 = weight\n        w2 = 1 - weight\n        return w1, w2\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=2, pretrained_url=None, mode='train'):\n\n        super(ResNet, self).__init__()\n        # orginal image\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        # filtered image\n        self.inplanes = 64\n        self.conv1_d = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1_d = nn.BatchNorm2d(64)\n        self.relu_d = nn.ReLU(inplace=True)\n        #         self.maxpool_d = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1_d = self._make_layer(block, 64, layers[0])\n        self.layer2_d = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3_d = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4_d = self._make_layer(block, 512, layers[3], stride=2)\n\n        ##### dcmaf fusion\n        self.fusion_2 = DCMAF(64, 32)\n        self.fusion_3 = DCMAF(128, 64)\n        self.fusion_4 = DCMAF(256, 128)\n        self.fusion_5 = DCMAF(512, 256)\n        ######\n\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 1024),\n            nn.ReLU(True),\n            nn.Linear(1024, 512),\n            nn.ReLU(True),\n            nn.Linear(512, 256),\n            nn.ReLU(True),\n            nn.Linear(256, num_classes)\n        )\n\n        if isinstance(pretrained_url, str) and mode == 'train':\n            self.pretrained_url = pretrained_url\n            self._load_resnet_pretrained()\n            print(\"********************** Pretrained model loaded **********************\")\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n    def _load_resnet_pretrained(self):\n        # pretrain_dict = torch.load(self.pretrained_path)\n        pretrain_dict = model_zoo.load_url(self.pretrained_url)\n        model_dict = {}\n        state_dict = self.state_dict()\n        for k, v in pretrain_dict.items():\n            if k in state_dict:\n                if k.startswith('conv1'):\n                    model_dict[k] = torch.mean(v, 1).data. \\\n                        view_as(state_dict[k])\n                    model_dict[k.replace('conv1', 'conv1_d')] = torch.mean(v, 1).data. \\\n                        view_as(state_dict[k.replace('conv1', 'conv1_d')])\n\n                elif k.startswith('bn1'):\n                    model_dict[k] = v\n                    model_dict[k.replace('bn1', 'bn1_d')] = v\n                elif k.startswith('layer'):\n                    model_dict[k] = v\n                    model_dict[k[:6] + '_d' + k[6:]] = v\n\n        state_dict.update(model_dict)\n        self.load_state_dict(state_dict)\n\n    def forward(self, x, f):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out_f = F.relu(self.bn1_d(self.conv1_d(f)))\n\n        out = self.layer1(out)\n        out_f = self.layer1_d(out_f)\n\n        w_i, w_f = self.fusion_2(out, out_f)\n        img_w = out.mul(w_i)\n        fil_w = out_f.mul(w_f)\n        out = out + fil_w\n        out_f = out_f + img_w\n        ###########\n\n        out = self.layer2(out)\n        out_f = self.layer2_d(out_f)\n\n        w_i, w_f = self.fusion_3(out, out_f)\n        img_w = out.mul(w_i)\n        fil_w = out_f.mul(w_f)\n        out = out + fil_w\n        out_f = out_f + img_w\n        ###########\n\n        out = self.layer3(out)\n        out_f = self.layer3_d(out_f)\n\n        w_i, w_f = self.fusion_4(out, out_f)\n        img_w = out.mul(w_i)\n        fil_w = out_f.mul(w_f)\n        out = out + fil_w\n        out_f = out_f + img_w\n        ###########\n\n        out = self.layer4(out)\n        out_f = self.layer4_d(out_f)\n\n        w_i, w_f = self.fusion_5(out, out_f)\n        img_w = out.mul(w_i)\n        fil_w = out_f.mul(w_f)\n        out = img_w + fil_w\n        ##########\n\n        out = self.pool(out)\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n","metadata":{"papermill":{"duration":0.028915,"end_time":"2025-04-13T23:42:48.817926","exception":false,"start_time":"2025-04-13T23:42:48.789011","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"2ccae0b0","cell_type":"code","source":"from torch.optim import SGD\n\ntorch.manual_seed(42)\ntorch.cuda.manual_seed_all(42)\n\nnum_epoch = 60\n\ndevice=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResNet(block=BasicBlock, layers=[3, 4, 6, 3], pretrained_url = model_urls['resnet34'])\n\nif torch.cuda.device_count()>1:\n    print(\"let use\",torch.cuda.device_count(),\"gpu\")\n    model=nn.DataParallel(model)\n\nmodel=model.to(device)\n\n# build optimizer and scheduler\noptimizer = SGD(model.parameters(), lr=0.006, momentum=0.9, weight_decay=0.0005)\nlambda1 = lambda epoch: ((1 - (epoch / num_epoch)) ** 0.9)\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda1)\n\nclass_weights = [len(train_data)/529, len(train_data)/277]\nclass_weights = torch.Tensor(class_weights).to(device)\n# Loss functions\ncriterion = nn.CrossEntropyLoss(weight=class_weights)","metadata":{"papermill":{"duration":1.497837,"end_time":"2025-04-13T23:42:50.318620","exception":false,"start_time":"2025-04-13T23:42:48.820783","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"ef40e808","cell_type":"code","source":"import time\nimport torch\nimport numpy as np\nimport sklearn.metrics as metric\nfrom tqdm import tqdm\n\n\n\nepoch_losses_train = []\nepoch_losses_val = []\nbest_f1_weighted, best_f1_binary = 0, 0\n\nstart = time.time()\nfor epoch in range(1, num_epoch+1):\n    print(\"epoch: %d/%d\" % (epoch, num_epoch))\n    ############################################################################\n    # train:\n    ############################################################################\n    \n    model.train()\n    batch_losses = []\n    for imgs, filter_img, label, _ in train_loader:\n        imgs = imgs.to(device)\n        filter_img = filter_img.to(device)\n        label = (label.type(torch.float32)).to(device)\n\n        optimizer.zero_grad()  # (reset gradients)\n        \n        out = model(imgs, filter_img)\n    \n        loss = criterion(out, label)\n    \n        loss_value = loss.data.detach().cpu().numpy()\n        batch_losses.append(loss_value)\n    \n        # optimization step:\n        \n        loss.backward()\n        optimizer.step()  # (perform optimization step)\n    \n    epoch_loss = np.mean(batch_losses)\n    epoch_losses_train.append(epoch_loss)\n    print(\"train loss: %g\" % epoch_loss)\n    \n    scheduler.step()\n    \n    ############################################################################\n    # val:\n    ############################################################################\n    t = []\n    p = []\n    if True:\n        model.eval()\n        batch_losses = []\n        for imgs, filter_img, label, _ in val_loader:\n            with torch.no_grad():\n                imgs = imgs.to(device)\n                filter_img = filter_img.to(device)\n                label = (label.type(torch.float32)).to(device)\n    \n                out = model(imgs, filter_img)\n    \n                preds = out.detach().max(dim=1)[1].cpu().numpy()\n                targets = label.detach().max(dim=1)[1].cpu().numpy()\n    \n                t.extend(targets)\n                p.extend(preds)\n    \n                # compute the loss:\n                loss = criterion(out, label)\n                loss_value = loss.data.cpu().numpy()\n                batch_losses.append(loss_value)\n    \n        epoch_loss = np.mean(batch_losses)\n        epoch_losses_val.append(epoch_loss)\n        print(\"val loss: %g\" % epoch_loss)\n    \n        f1_weighted = metric.f1_score(t, p, average='weighted')\n        print(\"f1 score: \", f1_weighted)\n        f1_binary = metric.f1_score(t, p, average='binary')\n        print(\"f1 binary: \", f1_binary)\n    \n    if f1_weighted > best_f1_weighted:\n        print(\"############ Best Result f1_weighted ############\")\n        print(metric.classification_report(t, p, zero_division=0.0))\n        out_filename = os.path.join('/kaggle/working/', f'best_f1_weighted_{mod}.pth')\n        torch.save(model.state_dict(), out_filename)\n        best_f1_weighted = f1_weighted\n    \n    if f1_binary > best_f1_binary:\n        print(\"############ Best Result f1_binary ############\")\n        print(metric.classification_report(t, p, zero_division=0.0))\n        out_filename = os.path.join('/kaggle/working/', f'best_f1_binary_{mod}.pth')\n        torch.save(model.state_dict(), out_filename)\n        best_f1_binary = f1_binary\n    \n    end = time.time()\n    forward_time = end - start\n    if forward_time > 18000:\n        print(f\"############ last=> epoch {epoch} time :{forward_time} ############\")\n        print(metric.classification_report(t, p, zero_division=0.0))\n        out_filename = os.path.join('/kaggle/working/',  f'last_weights_{mod}.pth')\n        torch.save(model.state_dict(), out_filename)\n        break\n","metadata":{"papermill":{"duration":1867.516797,"end_time":"2025-04-14T00:13:57.838801","exception":false,"start_time":"2025-04-13T23:42:50.322004","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"fe6b47d2","cell_type":"code","source":"a = np.arange(len(epoch_losses_val))\nfig, ax = plt.subplots()\ngloss = ax.plot(a, epoch_losses_train, label=\"train\")\ndloss = ax.plot(a, epoch_losses_val, label=\"val\")\n# ax.legend(handles=[gloss, dloss])\nfig.legend()\nplt.savefig(f'/kaggle/working/losses_plot_{mod}.png')","metadata":{"papermill":{"duration":0.289336,"end_time":"2025-04-14T00:13:58.137275","exception":false,"start_time":"2025-04-14T00:13:57.847939","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"20196189","cell_type":"code","source":"model.load_state_dict(torch.load(f'/kaggle/working/best_f1_weighted_{mod}.pth'))\nfig, axs = plt.subplots(2, 7, figsize=(30,30),layout=\"compressed\")\nsamples = random.choice(list(val_loader))\nwhile samples[0].shape[0] <14:\n    print(samples[0].shape[0])\n    samples = random.choice(list(val_loader))\n\naxs = axs.flatten()\nfor i in range(14):\n    imgs = samples[0][i].unsqueeze(dim=0)\n    filter_img = samples[1][i].unsqueeze(dim=0)\n    label = samples[2][i]\n    pred = model(imgs, filter_img)\n    pred = pred.detach().max(dim=1)[1].cpu().numpy()\n    axs[i].imshow(imgs[0].permute(1,2,0).detach().cpu().numpy(), cmap='gray')\n    axs[i].axis(\"off\")\n    axs[i].set_title(f\"label = {label.max(dim=0)[1].numpy()}, pred = {pred.item()}\", fontsize=20)\n    if i>=13:\n        break\n\nplt.savefig(f'/kaggle/working/PV_output_{mod}.png', bbox_inches='tight', pad_inches=0, dpi=400)","metadata":{"papermill":{"duration":10.722675,"end_time":"2025-04-14T00:14:08.869379","exception":false,"start_time":"2025-04-14T00:13:58.146704","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"e80191d1","cell_type":"code","source":"import time\nimport torch\nimport numpy as np\nimport sklearn.metrics as metric\nfrom tqdm import tqdm\n\nfrom torch.optim import SGD\n\nmod = 'poly'\ntrain_data = elpv(f\"/kaggle/input/elpv-and-pvel-ad-dataset/elpv_{mod}\", mode='train',types=mod)\nval_data = elpv(f\"/kaggle/input/elpv-and-pvel-ad-dataset/elpv_{mod}\", mode='val', types=mod)\ntrain_loader=torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True , num_workers = 4)\nval_loader=torch.utils.data.DataLoader(val_data, batch_size=64, shuffle=True , num_workers = 4)\n\ntorch.manual_seed(42)\ntorch.cuda.manual_seed_all(42)\n\nnum_epoch = 60\n\ndevice=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResNet(block=BasicBlock, layers=[3, 4, 6, 3], pretrained_url = model_urls['resnet34'])\n\nif torch.cuda.device_count()>1:\n    print(\"let use\",torch.cuda.device_count(),\"gpu\")\n    model=nn.DataParallel(model)\n\nmodel=model.to(device)\n\n# build optimizer and scheduler\noptimizer = SGD(model.parameters(), lr=0.006, momentum=0.9, weight_decay=0.0005)\nlambda1 = lambda epoch: ((1 - (epoch / num_epoch)) ** 0.9)\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda1)\n\nclass_weights = [len(train_data)/824, len(train_data)/339]\nclass_weights = torch.Tensor(class_weights).to(device)\n# Loss functions\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\nepoch_losses_train = []\nepoch_losses_val = []\nbest_f1_weighted, best_f1_binary = 0, 0\n\nstart = time.time()\nfor epoch in range(1, num_epoch+1):\n    print(\"epoch: %d/%d\" % (epoch, num_epoch))\n    ############################################################################\n    # train:\n    ############################################################################\n    \n    model.train()\n    batch_losses = []\n    for imgs, filter_img, label, _ in train_loader:\n        imgs = imgs.to(device)\n        filter_img = filter_img.to(device)\n        label = (label.type(torch.float32)).to(device)\n\n        optimizer.zero_grad()  # (reset gradients)\n        \n        out = model(imgs, filter_img)\n    \n        loss = criterion(out, label)\n    \n        loss_value = loss.data.detach().cpu().numpy()\n        batch_losses.append(loss_value)\n    \n        # optimization step:\n        \n        loss.backward()\n        optimizer.step()  # (perform optimization step)\n    \n    epoch_loss = np.mean(batch_losses)\n    epoch_losses_train.append(epoch_loss)\n    print(\"train loss: %g\" % epoch_loss)\n    \n    scheduler.step()\n    \n    ############################################################################\n    # val:\n    ############################################################################\n    t = []\n    p = []\n    if True:\n        model.eval()\n        batch_losses = []\n        for imgs, filter_img, label, _ in val_loader:\n            with torch.no_grad():\n                imgs = imgs.to(device)\n                filter_img = filter_img.to(device)\n                label = (label.type(torch.float32)).to(device)\n    \n                out = model(imgs, filter_img)\n    \n                preds = out.detach().max(dim=1)[1].cpu().numpy()\n                targets = label.detach().max(dim=1)[1].cpu().numpy()\n    \n                t.extend(targets)\n                p.extend(preds)\n    \n                # compute the loss:\n                loss = criterion(out, label)\n                loss_value = loss.data.cpu().numpy()\n                batch_losses.append(loss_value)\n    \n        epoch_loss = np.mean(batch_losses)\n        epoch_losses_val.append(epoch_loss)\n        print(\"val loss: %g\" % epoch_loss)\n    \n        f1_weighted = metric.f1_score(t, p, average='weighted')\n        print(\"f1 score: \", f1_weighted)\n        f1_binary = metric.f1_score(t, p, average='binary')\n        print(\"f1 binary: \", f1_binary)\n    \n    if f1_weighted > best_f1_weighted:\n        print(\"############ Best Result f1_weighted ############\")\n        print(metric.classification_report(t, p, zero_division=0.0))\n        out_filename = os.path.join('/kaggle/working/', f'best_f1_weighted_{mod}.pth')\n        torch.save(model.state_dict(), out_filename)\n        best_f1_weighted = f1_weighted\n    \n    if f1_binary > best_f1_binary:\n        print(\"############ Best Result f1_binary ############\")\n        print(metric.classification_report(t, p, zero_division=0.0))\n        out_filename = os.path.join('/kaggle/working/', f'best_f1_binary_{mod}.pth')\n        torch.save(model.state_dict(), out_filename)\n        best_f1_binary = f1_binary\n    \n    end = time.time()\n    forward_time = end - start\n    if forward_time > 24000:\n        print(f\"############ last=> epoch {epoch} time :{forward_time} ############\")\n        print(metric.classification_report(t, p, zero_division=0.0))\n        out_filename = os.path.join('/kaggle/working/',  f'last_weights_{mod}.pth')\n        torch.save(model.state_dict(), out_filename)\n        break\n\na = np.arange(len(epoch_losses_val))\nfig, ax = plt.subplots()\ngloss = ax.plot(a, epoch_losses_train, label=\"train\")\ndloss = ax.plot(a, epoch_losses_val, label=\"val\")\n# ax.legend(handles=[gloss, dloss])\nfig.legend()\nplt.savefig(f'/kaggle/working/losses_plot_{mod}.png')","metadata":{"papermill":{"duration":2717.949059,"end_time":"2025-04-14T00:59:26.851312","exception":false,"start_time":"2025-04-14T00:14:08.902253","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"2bf65e49","cell_type":"code","source":"model.load_state_dict(torch.load(f'/kaggle/working/best_f1_weighted_{mod}.pth'))\nfig, axs = plt.subplots(2, 7, figsize=(30,30),layout=\"compressed\")\nsamples = random.choice(list(val_loader))\nwhile samples[0].shape[0] <14:\n    print(samples[0].shape[0])\n    samples = random.choice(list(val_loader))\n\naxs = axs.flatten()\nfor i in range(14):\n    imgs = samples[0][i].unsqueeze(dim=0)\n    filter_img = samples[1][i].unsqueeze(dim=0)\n    label = samples[2][i]\n    pred = model(imgs, filter_img)\n    pred = pred.detach().max(dim=1)[1].cpu().numpy()\n    axs[i].imshow(imgs[0].permute(1,2,0).detach().cpu().numpy(), cmap='gray')\n    axs[i].axis(\"off\")\n    axs[i].set_title(f\"label = {label.max(dim=0)[1].numpy()}, pred = {pred.item()}\", fontsize=20)\n    if i>=13:\n        break\n\nplt.savefig(f'/kaggle/working/PV_output_{mod}.png', bbox_inches='tight', pad_inches=0, dpi=400)","metadata":{"papermill":{"duration":11.094367,"end_time":"2025-04-14T00:59:37.984626","exception":false,"start_time":"2025-04-14T00:59:26.890259","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"70df2b4d","cell_type":"code","source":"","metadata":{"papermill":{"duration":0.062899,"end_time":"2025-04-14T00:59:38.114235","exception":false,"start_time":"2025-04-14T00:59:38.051336","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}